# Guía de Implementación de Servidores de Aplicación y Base de Datos

**Proyecto:** QuickStay - Plataforma de Alquiler Express
**Módulo:** Administración de Sistemas Informáticos en Red (ASIR)
**Autor:** Antonio López Montes
**Fecha:** 23 de Diciembre de 2025

---

## 1. Servidores de Aplicación Java

Se implementarán dos servidores de aplicación Java redundantes en la VLAN de Servicios, configurados para trabajar detrás del balanceador de carga y conectarse a la base de datos MySQL.

**Servidor:** `appserver1.quickstay.local`
**IP:** `172.16.20.10`
**Servidor:** `appserver2.quickstay.local`
**IP:** `172.16.20.11`

### 1.1. Instalación de JRE/JDK y Servidor de Aplicaciones (Tomcat/WildFly)

Se instalará el entorno de ejecución Java (JRE) y el kit de desarrollo (JDK) junto con un servidor de aplicaciones como Apache Tomcat o WildFly (anteriormente JBoss).

```bash
# En ambos servidores de aplicación (appserver1 y appserver2)

# 1. Actualizar el sistema
sudo apt update && sudo apt upgrade -y

# 2. Instalar OpenJDK (JRE y JDK)
sudo apt install -y openjdk-17-jre openjdk-17-jdk

# 3. Instalar Apache Tomcat (ejemplo)
sudo apt install -y tomcat9 tomcat9-admin

# 4. Configurar usuarios de Tomcat (editar /etc/tomcat9/tomcat-users.xml)
# <role rolename="manager-gui"/>
# <role rolename="admin-gui"/>
# <user username="admin" password="your_tomcat_password" roles="manager-gui,admin-gui"/>

# 5. Reiniciar Tomcat
sudo systemctl restart tomcat9
```

### 1.2. Replicación de Sesiones (Clustering)

Para mantener la persistencia de las sesiones de usuario en un entorno balanceado, se configurará la replicación de sesiones entre los servidores de aplicación. Esto asegura que si un servidor falla, la sesión del usuario no se pierda.

```xml
<!-- En ambos servidores Tomcat, editar /etc/tomcat9/server.xml -->
<!-- Descomentar o añadir el siguiente elemento dentro de <Engine name="Catalina" defaultHost="localhost"> -->
<Cluster className="org.apache.catalina.ha.tcp.SimpleTcpCluster" channelSendOptions="8">
  <Manager className="org.apache.catalina.ha.session.BackupManager" expireSessionsOnShutdown="false" notifyListenersOnReplication="true"/>
  <Channel className="org.apache.catalina.tribes.group.GroupChannel">
    <Membership className="org.apache.catalina.tribes.membership.McastService" address="228.0.0.4" port="45564" frequency="500" dropTime="3000"/>
    <Receiver className="org.apache.catalina.tribes.transport.nio.NioReceiver" address="auto" port="4000" autoBind="100" selectorTimeout="5000" maxThreads="6"/>
    <Sender className="org.apache.catalina.tribes.transport.nio.PooledParallelSender"/>
    <Interceptor className="org.apache.catalina.tribes.group.interceptors.TcpFailureDetector"/>
    <Interceptor className="org.apache.catalina.tribes.group.interceptors.MessageDispatch15Interceptor"/>
  </Channel>
  <Valve className="org.apache.catalina.ha.tcp.ReplicationValve" filter="(.*)"/>
  <Valve className="org.apache.catalina.ha.session.JvmRouteBinderValve"/>
  <Deployer className="org.apache.catalina.ha.deploy.FarmWarDeployer"/>
  <ClusterListener className="org.apache.catalina.ha.session.JvmRouteSessionIDBinderListener"/>
  <ClusterListener className="org.apache.catalina.ha.session.ClusterSessionListener"/>
</Cluster>

<!-- Asegurarse de que el atributo jvmRoute esté configurado en el Engine o Host -->
<!-- <Engine name="Catalina" defaultHost="localhost" jvmRoute="appserver1"> -->
<!-- Para appserver1: jvmRoute="appserver1" -->
<!-- Para appserver2: jvmRoute="appserver2" -->

# Reiniciar Tomcat después de la configuración del cluster
sudo systemctl restart tomcat9
```

### 1.3. Conexión a Balanceador de Carga

Los servidores de aplicación se configurarán para escuchar en el puerto 8080 (o el puerto configurado para el balanceador) y responder a las peticiones reenviadas por el balanceador.

```xml
<!-- En ambos servidores Tomcat, asegurar que el conector HTTP esté configurado para el puerto 8080 -->
<!-- Editar /etc/tomcat9/server.xml -->
<Connector port="8080" protocol="HTTP/1.1"
           connectionTimeout="20000"
           redirectPort="8443" />
```

### 1.4. Despliegue de Aplicación Demo

Se desplegará una aplicación web de demostración (ej. un archivo `.war`) para verificar la funcionalidad del servidor de aplicaciones y la replicación de sesiones.

```bash
# En ambos servidores de aplicación

# 1. Copiar el archivo .war de la aplicación (ej. quickstay.war) al directorio de despliegue de Tomcat
sudo cp quickstay.war /var/lib/tomcat9/webapps/

# 2. Tomcat desplegará automáticamente la aplicación. Verificar en los logs.
# tail -f /var/log/tomcat9/catalina.out

# 3. Acceder a la aplicación a través del balanceador de carga (VIP) para verificar el despliegue.
```

---

## 2. Base de Datos MySQL HA

Se implementará una configuración de replicación Master-Slave para MySQL, garantizando la alta disponibilidad de los datos y la capacidad de recuperación ante desastres.

**Servidor:** `db1.quickstay.local` (Master)
**IP:** `172.16.20.20`
**Servidor:** `db2.quickstay.local` (Slave)
**IP:** `172.16.20.21`

### 2.1. Instalación de MySQL Master (db1)

```bash
# En db1.quickstay.local

# 1. Actualizar el sistema
sudo apt update && sudo apt upgrade -y

# 2. Instalar MySQL Server
sudo apt install -y mysql-server

# 3. Configurar MySQL para replicación Master
# Editar /etc/mysql/mysql.conf.d/mysqld.cnf
# Añadir o modificar las siguientes líneas en la sección [mysqld]
server-id               = 1
log_bin                 = /var/log/mysql/mysql-bin.log
binlog_do_db            = quickstay_db # Replicar solo esta base de datos
expire_logs_days        = 7
max_binlog_size         = 100M

# 4. Reiniciar MySQL
sudo systemctl restart mysql

# 5. Crear usuario de replicación y obtener el estado del Master
sudo mysql -u root -p
CREATE USER 'repl_user'@'%' IDENTIFIED BY 'repl_password';
GRANT REPLICATION SLAVE ON *.* TO 'repl_user'@'%';
FLUSH PRIVILEGES;

SHOW MASTER STATUS; # Anotar File y Position
```

### 2.2. Instalación de MySQL Slave (db2)

```bash
# En db2.quickstay.local

# 1. Actualizar el sistema e instalar MySQL Server (igual que db1)
sudo apt update && sudo apt upgrade -y
sudo apt install -y mysql-server

# 2. Configurar MySQL para replicación Slave
# Editar /etc/mysql/mysql.conf.d/mysqld.cnf
# Añadir o modificar las siguientes líneas en la sección [mysqld]
server-id               = 2
relay_log               = /var/log/mysql/mysql-relay-bin.log
read_only               = 1 # Opcional, para asegurar que no se escriba directamente en el Slave

# 3. Reiniciar MySQL
sudo systemctl restart mysql

# 4. Configurar el Slave para conectarse al Master
sudo mysql -u root -p
CHANGE MASTER TO MASTER_HOST='172.16.20.20',
MASTER_USER='repl_user',
MASTER_PASSWORD='repl_password',
MASTER_LOG_FILE='mysql-bin.000001', # Usar el File anotado de SHOW MASTER STATUS
MASTER_LOG_POS=123; # Usar el Position anotado de SHOW MASTER STATUS

START SLAVE;

# 5. Verificar el estado del Slave
SHOW SLAVE STATUS\G
```

### 2.3. Usuarios y Permisos Específicos

Se crearán usuarios con permisos mínimos necesarios para la aplicación y la administración.

```sql
# En db1 (Master)

# Usuario para la aplicación QuickStay
CREATE USER 'quickstay_app'@'172.16.20.10' IDENTIFIED BY 'app_password';
GRANT SELECT, INSERT, UPDATE, DELETE ON quickstay_db.* TO 'quickstay_app'@'172.16.20.10';
FLUSH PRIVILEGES;

# Usuario para monitoreo (ej. Zabbix)
CREATE USER 'monitor'@'172.16.30.20' IDENTIFIED BY 'monitor_password';
GRANT REPLICATION CLIENT ON *.* TO 'monitor'@'172.16.30.20';
FLUSH PRIVILEGES;
```

### 2.4. Scripts de Creación de BD Demo

Se proporcionará un script SQL para crear la base de datos `quickstay_db` y sus tablas de demostración.

```sql
-- create_quickstay_db.sql
CREATE DATABASE IF NOT EXISTS quickstay_db;
USE quickstay_db;

CREATE TABLE IF NOT EXISTS bookings (
    id INT AUTO_INCREMENT PRIMARY KEY,
    user_id INT NOT NULL,
    property_id INT NOT NULL,
    start_time DATETIME NOT NULL,
    end_time DATETIME NOT NULL,
    status VARCHAR(50) NOT NULL DEFAULT 'pending'
);

INSERT INTO bookings (user_id, property_id, start_time, end_time, status) VALUES
(1, 101, '2025-01-01 10:00:00', '2025-01-01 12:00:00', 'confirmed'),
(2, 102, '2025-01-02 14:00:00', '2025-01-02 16:00:00', 'pending');

-- Cargar el script en db1
sudo mysql -u root -p < create_quickstay_db.sql
```

---

## 3. Integración Completa 3-Capas

Esta fase se centra en verificar la conectividad y el flujo de datos a través de las tres capas (Web, Aplicación, Base de Datos) y optimizar las conexiones.

### 3.1. Conexión App → BD (Verificar)

Se verificará que los servidores de aplicación puedan conectarse correctamente a la base de datos MySQL.

| Prueba | Objetivo | Metodología | Criterio de Éxito |
| :--- | :--- | :--- | :--- |
| **Conexión DB** | Verificar que la aplicación puede acceder a la base de datos. | Configurar la aplicación para usar `db1.quickstay.local` (172.16.20.20) y realizar una consulta simple. | La aplicación debe conectarse a la DB y recuperar datos sin errores. |
| **Failover DB** | Verificar que la aplicación puede cambiar al Slave si el Master falla. | Detener `db1` y verificar que la aplicación puede conectarse a `db2` (después de la promoción manual del Slave). | La aplicación debe reconectarse a la DB activa. |

### 3.2. Conexión Web → App (Verificar)

Se verificará que el balanceador de carga redirige correctamente el tráfico a los servidores de aplicación.

| Prueba | Objetivo | Metodología | Criterio de Éxito |
| :--- | :--- | :--- | :--- |
| **Acceso Balanceado** | Verificar que las peticiones llegan a ambos servidores de aplicación. | Acceder a la aplicación a través de la VIP del balanceador y verificar los logs de acceso en `appserver1` y `appserver2`. | Las peticiones deben distribuirse entre ambos servidores. |

### 3.3. Pruebas Transaccionales Completas

Se realizarán pruebas de extremo a extremo para simular el flujo de trabajo completo de un usuario.

| Prueba | Objetivo | Metodología | Criterio de Éxito |
| :--- | :--- | :--- | :--- |
| **Flujo de Reserva** | Simular el proceso completo de reserva de una propiedad. | Utilizar una herramienta de prueba (ej. Selenium, Postman) para simular la navegación, selección, reserva y confirmación. | La reserva debe completarse con éxito y los datos deben reflejarse en la base de datos. |

### 3.4. Optimización de Conexiones

Se ajustarán los parámetros de conexión para mejorar el rendimiento y la eficiencia.

| Parámetro | Configuración | Justificación |
| :--- | :--- | :--- |
| **Pool de Conexiones** | Configurar un *pool* de conexiones en la aplicación (ej. HikariCP) | Reduce la sobrecarga de establecer nuevas conexiones a la DB para cada petición. |
| **Timeouts** | Ajustar *timeouts* de conexión y lectura en la aplicación y el balanceador. | Previene que las conexiones se queden colgadas indefinidamente, mejorando la resiliencia. |

---
**FIN DEL DOCUMENTO**
